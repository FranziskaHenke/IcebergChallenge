{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forked from https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os #mkdir, path.exists.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.python.org/3/library/warnings.html\n",
    "import warnings\n",
    "#don't print messages of future warings\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "#File paths\n",
    "INPUT_DIR=\"./input\"\n",
    "TRAIN_PATH=INPUT_DIR+\"/train.json\"\n",
    "TEST_PATH=INPUT_DIR+\"/test.json\"\n",
    "\n",
    "MODEL_DIR=\"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"{} doesn't exist. Creating...\".format(directory))\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "create_directory_if_not_exists(INPUT_DIR)\n",
    "create_directory_if_not_exists(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    '''data - train/test data as a pandas DataFrame'''\n",
    "    #Create 3 bands having HH, HV and avg of both\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_2\"]])\n",
    "    result = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data. NOTE: make sure you have downloaded and unziped the files\n",
    "train = pd.read_json(TRAIN_PATH)\n",
    "\n",
    "#transform it in the right format for training, applying data augmentation etc\n",
    "X_train = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation, GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our model\n",
    "def getModel():\n",
    "    #Building the model\n",
    "    model=Sequential()\n",
    "    #Conv Layer 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(75,75,3),padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    #Conv Layer 3\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' , padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 5\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 6\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    '''Returns an array of callbacks which will be called during the training of the model.'''\n",
    "    #stop training if there were no improvements after `patience` epochs\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    \n",
    "    #save the model after every epoch.\n",
    "    #save_best_only - do not override if the loss was higher than in the previous epoch.\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_file_or_create(file_path):\n",
    "    model=getModel()\n",
    "    #try:\n",
    "    #    model.load_weights(file_path)\n",
    "    #except OSError:\n",
    "    #    print(\"Could not load model from file: {}.\".format(file_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_train=train['is_iceberg']\n",
    "#split into train and validation data using 75% of the data as train data\n",
    "X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train, target_train, random_state=1, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 37, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 351,297\n",
      "Trainable params: 351,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1203 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "1203/1203 [==============================] - 47s 39ms/step - loss: 0.6292 - acc: 0.5902 - val_loss: 0.5942 - val_acc: 0.6958\n",
      "Epoch 2/20\n",
      "1203/1203 [==============================] - 44s 37ms/step - loss: 0.5887 - acc: 0.6750 - val_loss: 0.5819 - val_acc: 0.6384\n",
      "Epoch 3/20\n",
      "1203/1203 [==============================] - 44s 36ms/step - loss: 0.5616 - acc: 0.6800 - val_loss: 0.5554 - val_acc: 0.6683\n",
      "Epoch 4/20\n",
      "1203/1203 [==============================] - 39s 33ms/step - loss: 0.5060 - acc: 0.7199 - val_loss: 0.4896 - val_acc: 0.6908\n",
      "Epoch 5/20\n",
      "1203/1203 [==============================] - 42s 35ms/step - loss: 0.4479 - acc: 0.8005 - val_loss: 0.3979 - val_acc: 0.8180\n",
      "Epoch 6/20\n",
      "1203/1203 [==============================] - 51s 42ms/step - loss: 0.4394 - acc: 0.7955 - val_loss: 0.4191 - val_acc: 0.8105\n",
      "Epoch 7/20\n",
      "1203/1203 [==============================] - 47s 39ms/step - loss: 0.3911 - acc: 0.8213 - val_loss: 0.3775 - val_acc: 0.8529\n",
      "Epoch 8/20\n",
      "1203/1203 [==============================] - 48s 40ms/step - loss: 0.3704 - acc: 0.8379 - val_loss: 0.3053 - val_acc: 0.8803\n",
      "Epoch 9/20\n",
      "1203/1203 [==============================] - 44s 37ms/step - loss: 0.3555 - acc: 0.8512 - val_loss: 0.3181 - val_acc: 0.8628\n",
      "Epoch 10/20\n",
      "1203/1203 [==============================] - 42s 35ms/step - loss: 0.3161 - acc: 0.8712 - val_loss: 0.3074 - val_acc: 0.8653\n",
      "Epoch 11/20\n",
      "1203/1203 [==============================] - 43s 36ms/step - loss: 0.3380 - acc: 0.8537 - val_loss: 0.5123 - val_acc: 0.7855\n",
      "Epoch 12/20\n",
      "1203/1203 [==============================] - 38s 32ms/step - loss: 0.3342 - acc: 0.8587 - val_loss: 0.3316 - val_acc: 0.8728\n",
      "Epoch 13/20\n",
      "1203/1203 [==============================] - 38s 31ms/step - loss: 0.3142 - acc: 0.8562 - val_loss: 0.3062 - val_acc: 0.8678\n",
      "Epoch 14/20\n",
      "1203/1203 [==============================] - 38s 32ms/step - loss: 0.2845 - acc: 0.8786 - val_loss: 0.3488 - val_acc: 0.8479\n",
      "Epoch 15/20\n",
      "1203/1203 [==============================] - 38s 31ms/step - loss: 0.2711 - acc: 0.8894 - val_loss: 0.3842 - val_acc: 0.8329\n",
      "Epoch 16/20\n",
      "1203/1203 [==============================] - 38s 32ms/step - loss: 0.2827 - acc: 0.8761 - val_loss: 0.3327 - val_acc: 0.8678\n",
      "Epoch 17/20\n",
      "1203/1203 [==============================] - 38s 32ms/step - loss: 0.2514 - acc: 0.9002 - val_loss: 0.3068 - val_acc: 0.8653\n",
      "Epoch 18/20\n",
      "1203/1203 [==============================] - 38s 32ms/step - loss: 0.2314 - acc: 0.9036 - val_loss: 0.4299 - val_acc: 0.8454\n",
      "Epoch 19/20\n",
      "1203/1203 [==============================] - 38s 32ms/step - loss: 0.2415 - acc: 0.8969 - val_loss: 0.3036 - val_acc: 0.8579\n",
      "Epoch 20/20\n",
      "1203/1203 [==============================] - 38s 31ms/step - loss: 0.2189 - acc: 0.9144 - val_loss: 0.4238 - val_acc: 0.8628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee86053470>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras uses the HDF5 format \n",
    "file_path = MODEL_DIR+\"/model_new.h5\"\n",
    "#If the specified file path is a model, get it and continue training\n",
    "model=get_model_from_file_or_create(file_path)\n",
    "#callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "model.fit(X_train_cv, y_train_cv,\n",
    "          batch_size=24,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid))#,\n",
    "          #callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 [==============================] - 4s 10ms/step\n",
      "Test loss: 0.423774004874\n",
      "Test accuracy: 0.862842893511\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9627 samples, validate on 401 samples\n",
      "Epoch 1/4\n",
      "9627/9627 [==============================] - 298s 31ms/step - loss: 0.2525 - acc: 0.1326 - val_loss: 0.3976 - val_acc: 0.8329\n",
      "Epoch 2/4\n",
      "9627/9627 [==============================] - 295s 31ms/step - loss: 0.2286 - acc: 0.1336 - val_loss: 0.4259 - val_acc: 0.8479\n",
      "Epoch 3/4\n",
      "9627/9627 [==============================] - 296s 31ms/step - loss: 0.2209 - acc: 0.1341 - val_loss: 0.3686 - val_acc: 0.8479\n",
      "Epoch 4/4\n",
      "9627/9627 [==============================] - 298s 31ms/step - loss: 0.2173 - acc: 0.1343 - val_loss: 0.3603 - val_acc: 0.8703\n",
      "401/401 [==============================] - 4s 10ms/step\n",
      "Test loss: 0.360267382682\n",
      "Test accuracy: 0.870324189972\n"
     ]
    }
   ],
   "source": [
    "#pseudo labeling\n",
    "\n",
    "def pseudo_labeling( y_train_cv):\n",
    "    \n",
    "    test = pd.read_json(TEST_PATH)\n",
    "    X_test = prepare_data(test)\n",
    "    \n",
    "    #create labels for test data from existing model\n",
    "    Y_test_pred = model.predict(X_test, batch_size=24)\n",
    "    y_train_cv = y_train_cv.reshape(y_train_cv.shape[0], 1)\n",
    "\n",
    "    #combine existing training data with test data\n",
    "    x_combo = np.concatenate([X_train_cv, X_test])\n",
    "    y_combo = np.concatenate([y_train_cv, Y_test_pred])\n",
    "\n",
    "    #train model with pseudo data\n",
    "    model.fit(x_combo, y_combo,\n",
    "          verbose=1, batch_size=24, epochs=4, \n",
    "          validation_data=(X_valid, y_valid))\n",
    "\n",
    "    #print score\n",
    "    score = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "pseudo_labeling( y_train_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_and_submit(test_data, file_path=\"sub.csv\"):\n",
    "    '''test_data - pandas dataframe'''\n",
    "    \n",
    "    #transform it in the right format for the model, applying data augmentation etc\n",
    "    X_test = prepare_data(test_data)\n",
    "    \n",
    "    predicted_test=model.predict_proba(X_test)\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id']=test_data['id']\n",
    "    submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "    submission.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE the test data is about 2GB. Uncomment if you are sure you want to execute this\n",
    "#Load the test data.\n",
    "test = pd.read_json(TEST_PATH)\n",
    "make_predictions_and_submit(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
