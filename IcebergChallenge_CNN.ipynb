{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forked from https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os #mkdir, path.exists.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://docs.python.org/3/library/warnings.html\n",
    "import warnings\n",
    "#don't print messages of future warings\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "#File paths\n",
    "INPUT_DIR=\"./input\"\n",
    "TRAIN_PATH=INPUT_DIR+\"/train.json\"\n",
    "TEST_PATH=INPUT_DIR+\"/test.json\"\n",
    "\n",
    "MODEL_DIR=\"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models doesn't exist. Creating...\n"
     ]
    }
   ],
   "source": [
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"{} doesn't exist. Creating...\".format(directory))\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "create_directory_if_not_exists(INPUT_DIR)\n",
    "create_directory_if_not_exists(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    '''data - train/test data as a pandas DataFrame'''\n",
    "    #Create 3 bands having HH, HV and avg of both\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_2\"]])\n",
    "    result = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data. NOTE: make sure you have downloaded and unziped the files\n",
    "train = pd.read_json(TRAIN_PATH)\n",
    "\n",
    "#transform it in the right format for training, applying data augmentation etc\n",
    "X_train = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation, GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define our model\n",
    "def getModel():\n",
    "    #Building the model\n",
    "    model=Sequential()\n",
    "    #Conv Layer 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(75,75,3),padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    #Conv Layer 2\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' , padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    '''Returns an array of callbacks which will be called during the training of the model.'''\n",
    "    #stop training if there were no improvements after `patience` epochs\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    \n",
    "    #save the model after every epoch.\n",
    "    #save_best_only - do not override if the loss was higher than in the previous epoch.\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_from_file_or_create(file_path):\n",
    "    model=getModel()\n",
    "    try:\n",
    "        model.load_weights(file_path)\n",
    "    except OSError:\n",
    "        print(\"Could not load model from file: {}.\".format(file_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_train=train['is_iceberg']\n",
    "#split into train and validation data using 75% of the data as train data\n",
    "X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train, target_train, random_state=1, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 37, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 351,297\n",
      "Trainable params: 351,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Could not load model from file: ./models/model_v1.h5.\n",
      "Train on 1203 samples, validate on 401 samples\n",
      "Epoch 1/10\n",
      "1203/1203 [==============================] - 32s 27ms/step - loss: 0.6746 - acc: 0.5353 - val_loss: 0.6148 - val_acc: 0.5312\n",
      "Epoch 2/10\n",
      "1203/1203 [==============================] - 33s 27ms/step - loss: 0.5341 - acc: 0.6858 - val_loss: 0.6857 - val_acc: 0.7257\n",
      "Epoch 3/10\n",
      "1203/1203 [==============================] - 32s 26ms/step - loss: 0.5202 - acc: 0.7323 - val_loss: 0.5252 - val_acc: 0.7057\n",
      "Epoch 4/10\n",
      "1203/1203 [==============================] - 29s 24ms/step - loss: 0.4662 - acc: 0.7764 - val_loss: 0.3765 - val_acc: 0.8429\n",
      "Epoch 5/10\n",
      "1203/1203 [==============================] - 31s 26ms/step - loss: 0.4180 - acc: 0.8196 - val_loss: 0.4268 - val_acc: 0.8080\n",
      "Epoch 6/10\n",
      "1203/1203 [==============================] - 34s 29ms/step - loss: 0.3930 - acc: 0.8263 - val_loss: 0.3598 - val_acc: 0.8504\n",
      "Epoch 7/10\n",
      "1203/1203 [==============================] - 31s 25ms/step - loss: 0.3828 - acc: 0.8387 - val_loss: 0.8745 - val_acc: 0.6010\n",
      "Epoch 8/10\n",
      "1203/1203 [==============================] - 33s 27ms/step - loss: 0.3703 - acc: 0.8421 - val_loss: 0.3648 - val_acc: 0.8579\n",
      "Epoch 9/10\n",
      "1203/1203 [==============================] - 32s 26ms/step - loss: 0.3270 - acc: 0.8695 - val_loss: 0.3425 - val_acc: 0.8529\n",
      "Epoch 10/10\n",
      "1203/1203 [==============================] - 31s 26ms/step - loss: 0.3005 - acc: 0.8728 - val_loss: 0.4633 - val_acc: 0.8204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13256d278>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras uses the HDF5 format \n",
    "file_path = MODEL_DIR+\"/model_v1.h5\"\n",
    "#If the specified file path is a model, get it and continue training\n",
    "model=get_model_from_file_or_create(file_path)\n",
    "callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "model.fit(X_train_cv, y_train_cv,\n",
    "          batch_size=24,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 [==============================] - 3s 8ms/step\n",
      "Test loss: 0.463301382309\n",
      "Test accuracy: 0.820448878846\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2807 samples, validate on 401 samples\n",
      "Epoch 1/4\n",
      "2807/2807 [==============================] - 77s 27ms/step - loss: 0.3009 - acc: 0.3994 - val_loss: 0.3190 - val_acc: 0.8504\n",
      "Epoch 2/4\n",
      "2807/2807 [==============================] - 73s 26ms/step - loss: 0.3005 - acc: 0.3954 - val_loss: 0.3780 - val_acc: 0.8130\n",
      "Epoch 3/4\n",
      "2807/2807 [==============================] - 76s 27ms/step - loss: 0.2905 - acc: 0.4043 - val_loss: 0.4484 - val_acc: 0.8005\n",
      "Epoch 4/4\n",
      " 744/2807 [======>.......................] - ETA: 50s - loss: 0.2887 - acc: 0.4274"
     ]
    }
   ],
   "source": [
    "#pseudo labeling\n",
    "\n",
    "#create labels for test data from existing models\n",
    "Y_test_pred = model.predict(X_valid, batch_size=24)\n",
    "y_train_cv = y_train_cv.reshape(y_train_cv.shape[0], 1)\n",
    "\n",
    "#combine existing training data with test data\n",
    "x_combo = np.concatenate([X_train_cv, X_valid])\n",
    "y_combo = np.concatenate([y_train_cv, Y_test_pred])\n",
    "\n",
    "#train model to with pseudo data\n",
    "model.fit(x_combo, y_combo,\n",
    "          verbose=1, batch_size=24, epochs=4, \n",
    "             validation_data=(X_valid, y_valid))\n",
    "\n",
    "#print score\n",
    "score = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions_and_submit(test_data, file_path=\"sub.csv\"):\n",
    "    '''test_data - pandas dataframe'''\n",
    "    \n",
    "    #transform it in the right format for the model, applying data augmentation etc\n",
    "    X_test = prepare_data(test_data)\n",
    "    \n",
    "    predicted_test=model.predict_proba(X_test)\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id']=test_data['id']\n",
    "    submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "    submission.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOTE the test data is about 2GB. Uncomment if you are sure you want to execute this\n",
    "#Load the test data.\n",
    "#test = pd.read_json(TEST_PATH)\n",
    "#make_predictions_and_submit(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
