{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forked from https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os #mkdir, path.exists.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.python.org/3/library/warnings.html\n",
    "import warnings\n",
    "#don't print messages of future warings\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "#File paths\n",
    "INPUT_DIR=\"./input\"\n",
    "TRAIN_PATH=INPUT_DIR+\"/train.json\"\n",
    "TEST_PATH=INPUT_DIR+\"/test.json\"\n",
    "\n",
    "MODEL_DIR=\"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"{} doesn't exist. Creating...\".format(directory))\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "create_directory_if_not_exists(INPUT_DIR)\n",
    "create_directory_if_not_exists(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    '''data - train/test data as a pandas DataFrame'''\n",
    "    #Create 3 bands having HH, HV and avg of both\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_2\"]])\n",
    "    result = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data. NOTE: make sure you have downloaded and unziped the files\n",
    "train = pd.read_json(TRAIN_PATH)\n",
    "\n",
    "#transform it in the right format for training, applying data augmentation etc\n",
    "X_train = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation, GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our model\n",
    "def getModel():\n",
    "    #Building the model\n",
    "    model=Sequential()\n",
    "    #Conv Layer 1\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    '''Returns an array of callbacks which will be called during the training of the model.'''\n",
    "    #stop training if there were no improvements after `patience` epochs\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    \n",
    "    #save the model after every epoch.\n",
    "    #save_best_only - do not override if the loss was higher than in the previous epoch.\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_file_or_create(file_path):\n",
    "    model=getModel()\n",
    "    try:\n",
    "        model.load_weights(file_path)\n",
    "    except OSError:\n",
    "        print(\"Could not load model from file: {}.\".format(file_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_train=train['is_iceberg']\n",
    "#split into train and validation data using 75% of the data as train data\n",
    "X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train, target_train, random_state=1, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 73, 73, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 5, 64)          73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 560,193\n",
      "Trainable params: 560,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1203 samples, validate on 401 samples\n",
      "Epoch 1/1\n",
      "1203/1203 [==============================] - 26s 22ms/step - loss: 0.4813 - acc: 0.7589 - val_loss: 0.4545 - val_acc: 0.7955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e23d0de10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras uses the HDF5 format \n",
    "file_path = MODEL_DIR+\"/model_v1.h5\"\n",
    "#If the specified file path is a model, get it and continue training\n",
    "model=get_model_from_file_or_create(file_path)\n",
    "callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "model.fit(X_train_cv, y_train_cv,\n",
    "          batch_size=24,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 [==============================] - 2s 5ms/step\n",
      "Test loss: 0.45445804093544023\n",
      "Test accuracy: 0.7955112226883372\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_and_submit(test_data,file_path=\"sub.csv\"):\n",
    "    '''test_data - pandas dataframe'''\n",
    "    \n",
    "    #transform it in the right format for the model, applying data augmentation etc\n",
    "    X_test = prepare_data(test_data)\n",
    "    \n",
    "    predicted_test=model.predict_proba(X_test)\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id']=test_data['id']\n",
    "    submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "    submission.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE the test data is about 2GB. Uncomment if you are sure you want to execute this\n",
    "#Load the test data.\n",
    "#test = pd.read_json(TEST_PATH)\n",
    "#make_predictions_and_submit(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
