{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forked from https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os #mkdir, path.exists.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.python.org/3/library/warnings.html\n",
    "import warnings\n",
    "#don't print messages of future warings\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "#File paths\n",
    "INPUT_DIR=\"./input\"\n",
    "TRAIN_PATH=INPUT_DIR+\"/train.json\"\n",
    "TEST_PATH=INPUT_DIR+\"/test.json\"\n",
    "\n",
    "MODEL_DIR=\"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"{} doesn't exist. Creating...\".format(directory))\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "create_directory_if_not_exists(INPUT_DIR)\n",
    "create_directory_if_not_exists(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    '''data - train/test data as a pandas DataFrame'''\n",
    "    #Create 3 bands having HH, HV and avg of both\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_2\"]])\n",
    "    result = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data. NOTE: make sure you have downloaded and unziped the files\n",
    "train = pd.read_json(TRAIN_PATH)\n",
    "\n",
    "#transform it in the right format for training, applying data augmentation etc\n",
    "X_train = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation, GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our model\n",
    "def getModel():\n",
    "    #Building the model\n",
    "    model=Sequential()\n",
    "    #Conv Layer 1\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(75,75,3),padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    #Conv Layer 2\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' , padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    '''Returns an array of callbacks which will be called during the training of the model.'''\n",
    "    #stop training if there were no improvements after `patience` epochs\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    \n",
    "    #save the model after every epoch.\n",
    "    #save_best_only - do not override if the loss was higher than in the previous epoch.\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_file_or_create(file_path):\n",
    "    model=getModel()\n",
    "    try:\n",
    "        model.load_weights(file_path)\n",
    "    except OSError:\n",
    "        print(\"Could not load model from file: {}.\".format(file_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_train=train['is_iceberg']\n",
    "#split into train and validation data using 75% of the data as train data\n",
    "X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train, target_train, random_state=1, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Keras uses the HDF5 format \n",
    "file_path = MODEL_DIR+\"/model_v1.h5\"\n",
    "#If the specified file path is a model, get it and continue training\n",
    "model=get_model_from_file_or_create(file_path)\n",
    "callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "model.fit(X_train_cv, y_train_cv,\n",
    "          batch_size=24,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 [==============================] - 2s 5ms/step\n",
      "Test loss: 0.45445804093544023\n",
      "Test accuracy: 0.7955112226883372\n"
     ]
    }
   ],
   "source": [
    "#pseudo labeling\n",
    "\n",
    "#create labels for test data from existing models\n",
    "Y_test_pred = model.predict(X_test, batch_size=24)\n",
    "y_train_cv = y_train_cv.reshape(y_train_cv.shape[0], 1)\n",
    "\n",
    "#combine existing training data with test data\n",
    "x_combo = np.concatenate([X_train_cv, X_test])\n",
    "y_combo = np.concatenate([y_train_cv, Y_test_pred])\n",
    "\n",
    "#train model to with pseudo data\n",
    "model.fit(x_combo, y_combo,\n",
    "          verbose=1, batch_size=24, epochs=4, \n",
    "             validation_data=(X_valid, y_valid))\n",
    "\n",
    "#print score\n",
    "score = model.evaluate(X_valid, y_valid, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_and_submit(test_data,file_path=\"sub.csv\"):\n",
    "    '''test_data - pandas dataframe'''\n",
    "    \n",
    "    #transform it in the right format for the model, applying data augmentation etc\n",
    "    X_test = prepare_data(test_data)\n",
    "    \n",
    "    predicted_test=model.predict_proba(X_test)\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id']=test_data['id']\n",
    "    submission['is_iceberg']=predicted_test.reshape((predicted_test.shape[0]))\n",
    "    submission.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE the test data is about 2GB. Uncomment if you are sure you want to execute this\n",
    "#Load the test data.\n",
    "#test = pd.read_json(TEST_PATH)\n",
    "#make_predictions_and_submit(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
